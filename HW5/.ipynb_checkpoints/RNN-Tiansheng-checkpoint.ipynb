{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RNN LSTM VS GRU\n",
    "\n",
    "\n",
    "### 1. Data Download and Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) packages import and seed setup:\n",
    "    after setting the same seed, the result will be the same and thus repeatable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "# set the seed for reproduction\n",
    "SEED = 1234\n",
    "\n",
    "# set seed for torch process for either cpu or gpu devices\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-set the parameter for data downloading:\n",
    "\n",
    "1)  \"LABEL\" is the parameter that deal with sentiment, its tensor type should be float\n",
    "\n",
    "2)  \"TEXT\" is tokenized with spacy and in charge of the vocabulary part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(tensor_type=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading data-\"IMDB\" from online database in torchtext:\n",
    "\n",
    "1) split data into training set and test set\n",
    "\n",
    "2) training set also consists of data used for training and validation. So we need to subset them again into train and valid part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "train, valid = train.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then we want to build vocabulary with the top 25000 frequently used words:\n",
    "\n",
    "1) instead of using random initial settings for our words, we used pre-trained settings, which might lead us to better results in shorter time\n",
    "\n",
    "2) We specify and download all the vectors with several parameters: 6B means trained on 6 billion tokens. 100d means 100 demensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "create iterators:\n",
    "\n",
    "1) batch size is how much data passes through the network within each iteration\n",
    "\n",
    "2) After this step, data are sorted into pieces with same batch size. When iterator is called, it will return one batch from each part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, valid, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.text), \n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building the model\n",
    "\n",
    "Then we start the specification of two models, LSTM and GRU:\n",
    "\n",
    "General process is to take one-hot coded input vectors and put them into RNN models. Models will tranform them through several layers and trained the parameters based on the label given. \n",
    "\n",
    "\n",
    "### LSTM\n",
    "\n",
    "A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n",
    "\n",
    "The process of LSTM from Wikipedia is as following:\n",
    "  \n",
    " \\begin{split}\\begin{array}{ll}\n",
    "i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "c_t = f_t c_{(t-1)} + i_t \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "h_t = o_t \\tanh(c_t)\n",
    "\\end{array}\\end{split}\n",
    "\n",
    "where the initial values are $ c_{0}=0$ and $h_{0}=0$ \n",
    "\n",
    "h is the hidden state\n",
    "\n",
    "c is the cell state, which incorporates long and short term information\n",
    "\n",
    "x is the input vector\n",
    "\n",
    "i, f, g, o are the input, forget, cell, and output vectos. \n",
    "\n",
    "σ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        vocab_size: input dimension, dimension of one-hot vector, which is length of TEXT.vocab\n",
    "        embedding_dim: dimension of the word vector\n",
    "        hidden_dim: size of hidden states, hidden states are layers between input and output layers\n",
    "        output_dim: dimension of output class, we only need a real value 0-1 \n",
    "        n_layers: number of layers in the neural network, \n",
    "            output of hidden state in first layer is the input to the hidden state in the next layer\n",
    "        bidirectional: adds an extra layer that processes values from last to first, which is the essence of the LSTM algo\n",
    "        dropout: regularization to avoid overfitting, randomly dropout a node from the forward process, since\n",
    "        LSTM add more parameters, avoiding overfitting becomes super important\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM package that specifies embedings\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        # defines forward process, bidirectional requires the square of hidden dimension\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward defines the forwarding process\n",
    "        \"\"\"\n",
    "        \n",
    "        # regularization in the embedding process\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # output of the LSTM RNN process in each node, including the output, new hidden layer, and cell state\n",
    "        output, (hidden, cell) = self.rnn(embedded)        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        # regularize the hidden layer\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "        #hidden [batch size, hid. dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU\n",
    "\n",
    "GRU is also an RNN algorithm just the same as LSTM. Its performance has been shown pretty similar to LSTM model. Except it's much better on smaller dataset\n",
    "\n",
    "GRU in pytorch package do the following process:\n",
    "\n",
    "\n",
    "  \\begin{split}\\begin{array}{ll}\n",
    "r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
    "z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
    "n_t = \\tanh(W_{in} x_t + b_{in} + r_t (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
    "h_t = (1 - z_t) n_t + z_t h_{(t-1)} \\\\\n",
    "\\end{array}\\end{split}\n",
    "   \n",
    "   where h is the hidden layer\n",
    "   \n",
    "   x is the input. \n",
    "   \n",
    "   r, z, n are the reset, update, and new gates, respectively.\n",
    "   \n",
    "   σ is the sigmoid function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Using GRU package instead of LSTM in the LSTM class\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Again, the forwarding process of GRU algo. The difference here is that GRU does not have cell state,\n",
    "        as we can see from mathematical definition above\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid. dim]\n",
    "        #cell = [num layers * num directions, batch size, hid. dim]\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "                \n",
    "            \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations\n",
    "1. setup parameters\n",
    "2. Train models based on given parameters\n",
    "\n",
    "Specifications explanation can be seen in the inline comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify dimensions of different layers\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "# Define the random dropout rate for regularization\n",
    "DROPOUT = 0.5\n",
    "\n",
    "#Configure the model with inputs given above\n",
    "model_lstm = RNN_LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "model_gru = RNN_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "#Here we Check the size of pretrained embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign pretrained embeddings to embedding layer for GRU and LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1123,  0.3113,  0.3317,  ..., -0.4576,  0.6191,  0.5304],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.1123,  0.3113,  0.3317,  ..., -0.4576,  0.6191,  0.5304],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use Adam optimization algorithm from torch package\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer_lstm = optim.Adam(model_lstm.parameters())\n",
    "optimizer_gru = optim.Adam(model_gru.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our loss function: BCE with logits loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "#use GPU if availbale, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Set models with device configuration\n",
    "model_lstm = model_lstm.to(device)\n",
    "model_gru = model_gru.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary_accuracy function return the accuracy in percentage basis, to show how good the algorithm has done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as F\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    model: model that's going to be trained\n",
    "    optimizer: optimizer used to train\n",
    "    iterator: defined as above\n",
    "    \"\"\"\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # first zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # feed batch of sentences to model\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        # calculate gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        # return the times adjusted aggregated loss, times adjusted aggregated accuracty \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    similar to train fucntion, except main purpose is to evaluate the trained models\n",
    "    no need to zero gradients\n",
    "    just return the same times adjusted loss and accuracy on the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0429z\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-LSTM training data\n",
      "Epoch: 01, Train Loss: 0.643, Train Acc: 61.77%, Val. Loss: 0.620, Val. Acc: 50.98%\n",
      "RNN-LSTM training data\n",
      "Epoch: 02, Train Loss: 0.420, Train Acc: 81.71%, Val. Loss: 0.345, Val. Acc: 85.69%\n",
      "RNN-LSTM training data\n",
      "Epoch: 03, Train Loss: 0.272, Train Acc: 89.78%, Val. Loss: 0.290, Val. Acc: 89.24%\n",
      "RNN-LSTM training data\n",
      "Epoch: 04, Train Loss: 0.177, Train Acc: 93.71%, Val. Loss: 0.298, Val. Acc: 89.76%\n",
      "RNN-LSTM training data\n",
      "Epoch: 05, Train Loss: 0.128, Train Acc: 95.61%, Val. Loss: 0.325, Val. Acc: 89.16%\n"
     ]
    }
   ],
   "source": [
    "#number of epochs is set to be 5\n",
    "N_EPOCHS = 5\n",
    "\n",
    "#do 5 epochs and output the training and validation loss, accuracy\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss_lstm, train_acc_lstm = train_model(model_lstm, train_iterator, optimizer_lstm, criterion)\n",
    "    valid_loss_lstm, valid_acc_lstm = evaluate(model_lstm, valid_iterator, criterion)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"RNN-LSTM training data\")\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss_lstm:.3f}, Train Acc: {train_acc_lstm*100:.2f}%, Val. Loss: {valid_loss_lstm:.3f}, Val. Acc: {valid_acc_lstm*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0429z\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-LSTM test result\n",
      "Test Loss: 0.408, Test Acc: 86.77%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss_lstm, test_acc_lstm = evaluate(model_lstm, test_iterator, criterion)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"RNN-LSTM test result\")\n",
    "print(f'Test Loss: {test_loss_lstm:.3f}, Test Acc: {test_acc_lstm*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0429z\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-GRU training data\n",
      "Epoch: 01, Train Loss: 0.528, Train Acc: 71.23%, Val. Loss: 0.350, Val. Acc: 85.40%\n",
      "RNN-GRU training data\n",
      "Epoch: 02, Train Loss: 0.255, Train Acc: 89.64%, Val. Loss: 0.244, Val. Acc: 89.96%\n",
      "RNN-GRU training data\n",
      "Epoch: 03, Train Loss: 0.166, Train Acc: 93.70%, Val. Loss: 0.240, Val. Acc: 90.23%\n",
      "RNN-GRU training data\n",
      "Epoch: 04, Train Loss: 0.109, Train Acc: 96.20%, Val. Loss: 0.281, Val. Acc: 90.16%\n",
      "RNN-GRU training data\n",
      "Epoch: 05, Train Loss: 0.076, Train Acc: 97.39%, Val. Loss: 0.322, Val. Acc: 89.31%\n"
     ]
    }
   ],
   "source": [
    "#number of epochs is set to be 5\n",
    "N_EPOCHS = 5\n",
    "\n",
    "#do 5 epochs and output the training and validation loss, accuracy\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss_gru, train_acc_gru = train_model(model_gru, train_iterator, optimizer_gru, criterion)\n",
    "    valid_loss_gru, valid_acc_gru = evaluate(model_gru, valid_iterator, criterion)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"RNN-GRU training data\")\n",
    "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss_gru:.3f}, Train Acc: {train_acc_gru*100:.2f}%, Val. Loss: {valid_loss_gru:.3f}, Val. Acc: {valid_acc_gru*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test gru model and output accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0429z\\Anaconda3\\lib\\site-packages\\torchtext\\data\\field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-GRU test result\n",
      "Test Loss: 0.382, Test Acc: 87.47%\n"
     ]
    }
   ],
   "source": [
    "test_loss_gru, test_acc_gru = evaluate(model_gru, test_iterator, criterion)\n",
    "torch.cuda.empty_cache()\n",
    "print(\"RNN-GRU test result\")\n",
    "print(f'Test Loss: {test_loss_gru:.3f}, Test Acc: {test_acc_gru*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_lstm(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction_lstm = F.sigmoid(model_lstm(tensor))\n",
    "    return prediction_lstm.item()\n",
    "\n",
    "def predict_sentiment_gru(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction_gru = F.sigmoid(model_gru(tensor))\n",
    "    return prediction_gru.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8921913504600525"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_lstm(\"The result is hugely enjoyable, and hooray for Hollywood for making it happen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003819111967459321"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_lstm(\"A disordered and unfocused ghost story that bears all the very worst habits of the genre.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9169970154762268"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_gru(\"The result is hugely enjoyable, and hooray for Hollywood for making it happen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019471831619739532"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment_gru(\"A disordered and unfocused ghost story that bears all the very worst habits of the genre.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
